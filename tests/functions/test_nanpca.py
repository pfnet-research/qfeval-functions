import math

import numpy as np
import pytest
import torch
from sklearn.decomposition import PCA

import qfeval_functions
import qfeval_functions.functions as QF


def test_nanpca() -> None:
    """Tests if the components generated by qfeval.pca corresponds to the
    result of sklearn.
    """
    qfeval_functions.random.seed()
    factors = QF.randn(100, 10) * QF.randn(10).exp()
    returns = torch.mm(factors, QF.randn(10, 10))
    d = PCA()
    d.fit(returns.numpy())
    pca_result = QF.nanpca(returns)
    expected = d.components_
    np.testing.assert_array_almost_equal(
        np.abs(pca_result.components.numpy()), np.abs(expected), decimal=3
    )
    np.testing.assert_array_almost_equal(
        np.abs(pca_result.explained_variance.numpy()),
        d.explained_variance_,
        decimal=3,
    )
    # Test if applying PCA to a batch of matrices works.
    batch_returns = returns[None, None].expand(3, 4, 100, 10)
    batch_result = QF.nanpca(batch_returns).components
    assert batch_result.shape == (3, 4, 10, 10)
    np.testing.assert_array_almost_equal(
        np.abs(batch_result.numpy()[0, 0]),
        np.abs(expected),
        decimal=3,
    )


def test_nanpca_with_nans() -> None:
    """Tests if nanpca estimates better than just filling NaNs with zeros."""
    qfeval_functions.random.seed()
    failed, succeeded = 0, 0
    for _ in range(100):
        factors = QF.randn(1000, 10) * QF.randn(10).exp()
        returns = torch.mm(factors, QF.randn(10, 10))
        d = PCA()
        d.fit(returns.numpy())
        expected = np.abs(d.components_)
        returns_with_nans = torch.where(
            QF.rand_like(returns) < 0.1,
            torch.as_tensor(math.nan),
            returns,
        )
        actual_nan_to_num = np.abs(
            QF.nanpca(returns_with_nans.nan_to_num()).components.numpy()
        )
        error_nan_to_num = (expected - actual_nan_to_num).std()
        actual_nanpca = np.abs(QF.nanpca(returns_with_nans).components.numpy())
        error_nanpca = (expected - actual_nanpca).std()
        if error_nanpca < error_nan_to_num:
            succeeded += 1
        else:
            failed += 1
    assert failed / (failed + succeeded) <= 0.1


def test_nanpca_basic_functionality() -> None:
    """Test basic NaN-aware PCA functionality."""
    # Simple 2D case - create data with known structure
    torch.manual_seed(42)
    x = torch.randn(50, 3)
    # Make first component dominant
    x[:, 0] = x[:, 0] * 3

    result = QF.nanpca(x)

    # Check output structure
    assert isinstance(result.components, torch.Tensor)
    assert isinstance(result.explained_variance, torch.Tensor)
    assert result.components.shape == (3, 3)
    assert result.explained_variance.shape == (3,)

    # Explained variance should be sorted in descending order
    assert torch.all(
        result.explained_variance[:-1] >= result.explained_variance[1:]
    )


def test_nanpca_return_type() -> None:
    """Test that nanpca returns proper dataclass."""
    x = torch.randn(20, 4)
    result = QF.nanpca(x)

    # Check it's the right type
    assert hasattr(result, "components")
    assert hasattr(result, "explained_variance")
    assert isinstance(result, QF.nanpca(x).__class__)


def test_nanpca_shape_preservation() -> None:
    """Test that nanpca handles different input shapes correctly."""
    # 2D input
    x_2d = torch.randn(30, 5)
    result_2d = QF.nanpca(x_2d)
    assert result_2d.components.shape == (5, 5)
    assert result_2d.explained_variance.shape == (5,)

    # 3D batch input
    x_3d = torch.randn(2, 30, 5)
    result_3d = QF.nanpca(x_3d)
    assert result_3d.components.shape == (2, 5, 5)
    assert result_3d.explained_variance.shape == (2, 5)

    # 4D batch input
    x_4d = torch.randn(2, 3, 30, 5)
    result_4d = QF.nanpca(x_4d)
    assert result_4d.components.shape == (2, 3, 5, 5)
    assert result_4d.explained_variance.shape == (2, 3, 5)


def test_nanpca_with_nan_values() -> None:
    """Test nanpca with NaN values in input."""
    torch.manual_seed(42)
    x = torch.randn(100, 4)

    # Add some NaN values
    nan_mask = torch.rand_like(x) < 0.1
    x_with_nan = torch.where(nan_mask, math.nan, x)

    result = QF.nanpca(x_with_nan)

    # Should still produce valid output
    assert result.components.shape == (4, 4)
    assert result.explained_variance.shape == (4,)

    # No NaN in outputs
    assert torch.isfinite(result.components).all()
    assert torch.isfinite(result.explained_variance).all()

    # Explained variance should be non-negative and sorted
    assert torch.all(result.explained_variance >= 0)
    assert torch.all(
        result.explained_variance[:-1] >= result.explained_variance[1:]
    )


def test_nanpca_all_nan_column() -> None:
    """Test nanpca behavior with columns that are all NaN."""
    x = torch.randn(50, 4)
    # Make one column all NaN
    x[:, 2] = math.nan

    result = QF.nanpca(x)

    # Should still work and produce finite results
    assert result.components.shape == (4, 4)
    assert result.explained_variance.shape == (4,)

    # Results should be finite
    assert torch.isfinite(result.components).all()
    assert torch.isfinite(result.explained_variance).all()


def test_nanpca_mathematical_properties() -> None:
    """Test mathematical properties of PCA."""
    torch.manual_seed(42)
    x = torch.randn(100, 4)
    result = QF.nanpca(x)

    # Components should be orthonormal (orthogonal and unit norm)
    components = result.components

    # Check orthogonality
    gram_matrix = torch.mm(components, components.t())
    identity = torch.eye(components.shape[0], dtype=components.dtype)
    torch.testing.assert_close(gram_matrix, identity, atol=1e-5, rtol=1e-5)

    # Check unit norm
    norms = torch.norm(components, dim=1)
    expected_norms = torch.ones_like(norms)
    torch.testing.assert_close(norms, expected_norms, atol=1e-5, rtol=1e-5)


def test_nanpca_explained_variance_properties() -> None:
    """Test properties of explained variance."""
    torch.manual_seed(42)
    x = torch.randn(80, 5)
    result = QF.nanpca(x)

    # Explained variance should be non-negative
    assert torch.all(result.explained_variance >= 0)

    # Should be in descending order
    sorted_variance, _ = torch.sort(result.explained_variance, descending=True)
    torch.testing.assert_close(result.explained_variance, sorted_variance)

    # Total explained variance should be close to trace of covariance matrix
    cov_matrix = torch.cov(x.t())
    total_variance = torch.trace(cov_matrix)
    explained_total = result.explained_variance.sum()

    # Should be close (within numerical precision)
    torch.testing.assert_close(
        explained_total, total_variance, atol=1e-4, rtol=1e-4
    )


def test_nanpca_single_feature() -> None:
    """Test nanpca with single feature."""
    x = torch.randn(50, 1)
    result = QF.nanpca(x)

    assert result.components.shape == (1, 1)
    assert result.explained_variance.shape == (1,)

    # Single component should be [1] or [-1]
    assert torch.abs(result.components[0, 0]).item() == pytest.approx(
        1.0, abs=1e-5
    )


def test_nanpca_identical_features() -> None:
    """Test nanpca with identical features."""
    x = torch.randn(50, 1)
    # Create identical columns
    x_identical = x.repeat(1, 3)

    result = QF.nanpca(x_identical)

    assert result.components.shape == (3, 3)
    assert result.explained_variance.shape == (3,)

    # Only first component should have significant variance
    assert result.explained_variance[0] > 0
    # Other components should have near-zero variance
    assert torch.all(result.explained_variance[1:] < 1e-5)


def test_nanpca_zero_variance_feature() -> None:
    """Test nanpca with zero variance feature."""
    x = torch.randn(50, 3)
    # Make one feature constant (zero variance)
    x[:, 1] = 5.0

    result = QF.nanpca(x)

    assert result.components.shape == (3, 3)
    assert result.explained_variance.shape == (3,)

    # Should handle zero variance gracefully
    assert torch.isfinite(result.components).all()
    assert torch.isfinite(result.explained_variance).all()


def test_nanpca_batch_processing() -> None:
    """Test nanpca with batch processing."""
    torch.manual_seed(42)
    # Create batch of different datasets
    batch_size = 3
    x_batch = torch.randn(batch_size, 40, 4)

    result_batch = QF.nanpca(x_batch)

    # Check shapes
    assert result_batch.components.shape == (batch_size, 4, 4)
    assert result_batch.explained_variance.shape == (batch_size, 4)

    # Compare with individual processing
    for i in range(batch_size):
        result_individual = QF.nanpca(x_batch[i])

        # Should be very close (allowing for numerical differences)
        torch.testing.assert_close(
            result_batch.components[i],
            result_individual.components,
            atol=1e-5,
            rtol=1e-5,
        )
        torch.testing.assert_close(
            result_batch.explained_variance[i],
            result_individual.explained_variance,
            atol=1e-5,
            rtol=1e-5,
        )


def test_nanpca_numerical_stability() -> None:
    """Test numerical stability with various scales."""
    # Test with very small values
    x_small = torch.randn(50, 3) * 1e-6
    result_small = QF.nanpca(x_small)
    assert torch.isfinite(result_small.components).all()
    assert torch.isfinite(result_small.explained_variance).all()

    # Test with very large values
    x_large = torch.randn(50, 3) * 1e6
    result_large = QF.nanpca(x_large)
    assert torch.isfinite(result_large.components).all()
    assert torch.isfinite(result_large.explained_variance).all()


def test_nanpca_reproducibility() -> None:
    """Test that nanpca produces consistent results."""
    torch.manual_seed(123)
    x = torch.randn(60, 4)
    # Add some NaN values
    x[x < -1.5] = math.nan

    result1 = QF.nanpca(x)
    result2 = QF.nanpca(x)

    torch.testing.assert_close(result1.components, result2.components)
    torch.testing.assert_close(
        result1.explained_variance, result2.explained_variance
    )


def test_nanpca_high_dimensional() -> None:
    """Test nanpca with higher dimensional data."""
    # Test with more features than samples
    x = torch.randn(30, 50)
    result = QF.nanpca(x)

    assert result.components.shape == (50, 50)
    assert result.explained_variance.shape == (50,)

    # Many components should have zero or near-zero variance
    n_nonzero = (result.explained_variance > 1e-6).sum()
    # The exact number may vary due to numerical precision, just check it's reasonable
    assert n_nonzero <= x.shape[1]  # At most n_features
    assert n_nonzero >= min(
        x.shape[0] - 1, 1
    )  # At least some non-zero components


def test_nanpca_centering() -> None:
    """Test that PCA properly centers the data."""
    # Create data with non-zero mean
    torch.manual_seed(42)
    x = torch.randn(100, 3) + torch.tensor([10.0, -5.0, 2.0])

    result = QF.nanpca(x)

    # PCA should work regardless of the mean
    assert torch.isfinite(result.components).all()
    assert torch.isfinite(result.explained_variance).all()

    # Components should still be orthonormal
    gram_matrix = torch.mm(result.components, result.components.t())
    identity = torch.eye(3, dtype=result.components.dtype)
    torch.testing.assert_close(gram_matrix, identity, atol=1e-4, rtol=1e-4)


def test_nanpca_performance() -> None:
    """Test nanpca performance with moderately large data."""
    # Test with reasonably large dataset
    x = torch.randn(500, 20)
    # Add some NaN values
    x[torch.rand_like(x) < 0.05] = math.nan

    result = QF.nanpca(x)

    assert result.components.shape == (20, 20)
    assert result.explained_variance.shape == (20,)
    assert torch.isfinite(result.components).all()
    assert torch.isfinite(result.explained_variance).all()


def test_nanpca_edge_cases() -> None:
    """Test nanpca edge cases."""
    # Very small dataset
    x_small = torch.randn(3, 2)
    result_small = QF.nanpca(x_small)
    assert result_small.components.shape == (2, 2)
    assert result_small.explained_variance.shape == (2,)

    # Single sample (this might not work, but test behavior)
    # TODO(claude): The nanpca function should handle edge cases more gracefully.
    # Expected behavior: when there are insufficient samples (n_samples < n_features),
    # the function should either raise a clear error with explanation, or automatically
    # adjust to return only the possible principal components (min(n_samples-1, n_features)).
    try:
        x_single = torch.randn(1, 3)
        result_single = QF.nanpca(x_single)
        # If it works, check output
        assert result_single.components.shape == (3, 3)
        assert result_single.explained_variance.shape == (3,)
    except Exception:
        # It's acceptable if single sample doesn't work
        pass


def test_nanpca_gradient_compatibility() -> None:
    """Test that nanpca works with gradient computation."""
    # Note: nanpca may not support gradients due to eigenvalue decomposition
    # This test verifies the behavior rather than requiring gradient support
    x = torch.randn(50, 3, requires_grad=True)

    try:
        result = QF.nanpca(x)
        # If it works, try to compute gradients
        loss = result.explained_variance.sum()
        loss.backward()

        assert x.grad is not None
        assert x.grad.shape == x.shape
    except RuntimeError as e:
        # It's acceptable if gradients aren't supported due to numpy conversion
        if "requires grad" in str(e) or "numpy" in str(e):
            # This is expected behavior - eigenvalue decomposition often doesn't support gradients
            pass
        else:
            raise


def test_nanpca_comparison_benefits() -> None:
    """Test that nanpca provides benefits over simple imputation."""
    torch.manual_seed(42)

    # Create data with structure
    true_components = torch.randn(5, 3)
    loadings = torch.randn(100, 5)
    x_clean = torch.mm(loadings, true_components)

    # Add noise
    x_clean += torch.randn_like(x_clean) * 0.1

    # Create version with NaNs
    x_with_nan = x_clean.clone()
    nan_mask = torch.rand_like(x_with_nan) < 0.2
    x_with_nan[nan_mask] = math.nan

    # Compare nanpca vs simple zero-filling
    result_nanpca = QF.nanpca(x_with_nan)
    result_zerofill = QF.nanpca(x_with_nan.nan_to_num(0))

    # Both should produce valid results
    assert torch.isfinite(result_nanpca.components).all()
    assert torch.isfinite(result_zerofill.components).all()

    # nanpca explained variance should generally be more meaningful
    assert torch.all(result_nanpca.explained_variance >= 0)
    assert torch.all(result_zerofill.explained_variance >= 0)
